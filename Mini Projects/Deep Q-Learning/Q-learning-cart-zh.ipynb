{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度 $Q$-学习\n",
    "\n",
    "在此 notebook 中，我们将构建一个可以通过强化学习学会玩游戏的神经网络。具体而言，我们将使用 $Q$-学习训练智能体玩一个叫做 [Cart-Pole](https://gym.openai.com/envs/CartPole-v0) 的游戏。在此游戏中，小车上有一个可以自由摆动的杆子。小车可以向左和向右移动，目标是尽量长时间地使杆子保持笔直。\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "我们可以使用 [OpenAI Gym](https://github.com/openai/gym) 模拟该游戏。首先，我们看看 OpenAI Gym 的原理。然后，我们将训练智能体玩 Cart-Pole 游戏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2018-01-22 23:10:02,350] Making new env: CartPole-v1\n",
    "\n",
    "\n",
    "Number of possible actions: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过 `env` 与模拟环境互动。你可以通过 `env.action_space.n`查看有多少潜在的动作，并且使用 `env.action_space.sample()` 获得随机动作。向 `env.step` 传入动作（用整数表示）将生成模拟环境的下一个步骤。所有 Gym 游戏基本都是这样。\n",
    "\n",
    "在 Cart-Pole 游戏中有两个潜在动作，即使小车向左或向右移动。因此我们可以采取两种动作，分别表示为 0 和 1。\n",
    "\n",
    "运行以下代码以与环境互动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [] # actions that the agent selects\n",
    "rewards = [] # obtained rewards\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()  # choose a random action\n",
    "    state, reward, done, _ = env.step(action) \n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以查看动作和奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actions:', actions)\n",
    "print('Rewards:', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
    "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当杆子倾斜角度超过特定的角度之后，游戏就会重置。当游戏还在运行时，在每一步都会返回奖励 1.0。游戏运行时间越久，我们获得的奖励就越多。网络的目标是通过使杆子保持垂直状态最大化奖励。为此，它将使小车向左和向右移动。\n",
    "\n",
    "## $Q$-网络\n",
    "\n",
    "为了跟踪动作值，我们将使用接受状态 $s$ 作为输入的神经网络。输出将是每个潜在动作的 $Q$ 值（即输出是_输入状态 $s$ 对应的_**所有**动作值 $Q(s,a)$。\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "对于这个 Cart-Pole 游戏，状态有四个值：小车的位置和速度，杆子的位置和速度。因此，该神经网络有**四个输入**（状态中的每个值对应一个输入）和**两个输出**（每个潜在动作对应一个输出）。\n",
    "\n",
    "正如在这节课所讨论的，为了实现训练目标，我们首先将利用状态  $s$ 提供的背景信息选择动作 $a$，然后使用该动作模拟游戏。这样将会获得下个状态 $s'$ 以及奖励 $r$。这样我们就可以计算 $\\hat{Q}(s,a) = r + \\gamma \\max_{a'}{Q(s', a')}$。然后，我们通过最小化 $(\\hat{Q}(s,a) - Q(s,a))^2$ 更新权重。\n",
    "\n",
    "下面是 $Q$ 网络的一种实现。它使用两个包含 ReLU 激活函数的完全连接层。两层似乎很好，三层可能更好，你可以随意尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经验回放\n",
    "\n",
    "强化学习算法可能会因为状态之间存在关联性而出现稳定性问题。为了在训练期间减少关联性，我们可以存储智能体的经验，稍后从这些经验中随机抽取一个小批量经验进行训练。 \n",
    "\n",
    "在以下代码单元格中，我们将创建一个 `Memory` 对象来存储我们的经验，即转换 $<s, a, r, s'>$。该存储器将设有最大容量，以便保留更新的经验并删除旧的经验。然后，我们将随机抽取一个小批次转换 $<s, a, r, s'>$ 并用它来训练智能体。\n",
    "\n",
    "我在下面实现了 `Memory` 对象。如果你不熟悉 `deque`，其实它是一个双端队列。可以将其看做在两端都有开口的管子。你可以从任何一端放入物体。但是如果放满了，再添加物体的话将使物体从另一端被挤出。这是一种非常适合内存缓冲区的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-学习训练算法\n",
    "\n",
    "我们将使用以下算法训练网络。对于此游戏，目标是使杆子在 195 帧内都保持垂直状态。因此当我们满足该目标后，可以开始新的阶段。如果杆子倾斜角度太大，或者小车向左或向右移动幅度太大，则游戏结束。当游戏结束后，我们可以开始新的阶段。现在，为了训练智能体：\n",
    "\n",
    "* 初始化存储器 $D$\n",
    "* 使用随机权重初始化动作值网络 $Q$\n",
    "* **对于**阶段 $\\leftarrow 1$ **到** $M$，**执行**以下操作\n",
    "  * 观察 $s_0$\n",
    "  * **对于** $t \\leftarrow 0$ **到** $T-1$，**执行**以下操作\n",
    "     * 对于概率 $\\epsilon$，选择随机动作 $a_t$，否则选择 $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "     * 在模拟器中执行动作 $a_t$，并观察奖励 $r_{t+1}$ 和新状态 $s_{t+1}$\n",
    "     * 将转换 $<s_t, a_t, r_{t+1}, s_{t+1}>$ 存储在存储器 $D$ 中\n",
    "     * 从 $D$: $<s_j, a_j, r_j, s'_j>$ 中随机抽取小批量经验\n",
    "     * 如果阶段在 $j+1$ 时结束，设为 $\\hat{Q}_j = r_j$，否则设为 $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * 创建梯度下降步骤，损失为 $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**\n",
    "\n",
    "建议你花时间扩展这段代码，以实现我们在这节课讨论的一些改进之处，从而包含固定 $Q$ 目标、双 DQN、优先回放和/或对抗网络。\n",
    "\n",
    "## 超参数\n",
    "\n",
    "对于强化学习，比较难的一个方面是超参数很大。我们不仅要调整网络，还要调整模拟环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充经验存储器\n",
    "\n",
    "我们在下面重新初始化了模拟环境并提前填充了存储器。智能体正在采取随机动作，并将转换存储在存储器中。这样可以帮助智能体探索该游戏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "下面我们将训练智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode: 1 Total reward: 13.0 Training loss: 1.0202 Explore P: 0.9987\n",
    "Episode: 2 Total reward: 13.0 Training loss: 1.0752 Explore P: 0.9974\n",
    "Episode: 3 Total reward: 9.0 Training loss: 1.0600 Explore P: 0.9965\n",
    "Episode: 4 Total reward: 17.0 Training loss: 1.0429 Explore P: 0.9949\n",
    "Episode: 5 Total reward: 16.0 Training loss: 1.0519 Explore P: 0.9933\n",
    "Episode: 6 Total reward: 15.0 Training loss: 1.0574 Explore P: 0.9918\n",
    "Episode: 7 Total reward: 12.0 Training loss: 1.0889 Explore P: 0.9906\n",
    "Episode: 8 Total reward: 27.0 Training loss: 1.0859 Explore P: 0.9880\n",
    "Episode: 9 Total reward: 24.0 Training loss: 1.2007 Explore P: 0.9857\n",
    "Episode: 10 Total reward: 17.0 Training loss: 1.1116 Explore P: 0.9840\n",
    "Episode: 11 Total reward: 12.0 Training loss: 1.0739 Explore P: 0.9828\n",
    "Episode: 12 Total reward: 25.0 Training loss: 1.0805 Explore P: 0.9804\n",
    "Episode: 13 Total reward: 23.0 Training loss: 1.0628 Explore P: 0.9782\n",
    "Episode: 14 Total reward: 31.0 Training loss: 1.0248 Explore P: 0.9752\n",
    "Episode: 15 Total reward: 15.0 Training loss: 0.9859 Explore P: 0.9737\n",
    "Episode: 16 Total reward: 12.0 Training loss: 1.0983 Explore P: 0.9726\n",
    "Episode: 17 Total reward: 16.0 Training loss: 1.4343 Explore P: 0.9710\n",
    "Episode: 18 Total reward: 21.0 Training loss: 1.2696 Explore P: 0.9690\n",
    "Episode: 19 Total reward: 15.0 Training loss: 1.3542 Explore P: 0.9676\n",
    "Episode: 20 Total reward: 15.0 Training loss: 1.2635 Explore P: 0.9661\n",
    "Episode: 21 Total reward: 16.0 Training loss: 1.3648 Explore P: 0.9646\n",
    "Episode: 22 Total reward: 43.0 Training loss: 1.6088 Explore P: 0.9605\n",
    "Episode: 23 Total reward: 7.0 Training loss: 1.5027 Explore P: 0.9599\n",
    "Episode: 24 Total reward: 13.0 Training loss: 1.7275 Explore P: 0.9586\n",
    "Episode: 25 Total reward: 18.0 Training loss: 1.3902 Explore P: 0.9569\n",
    "Episode: 26 Total reward: 27.0 Training loss: 2.5874 Explore P: 0.9544\n",
    "Episode: 27 Total reward: 32.0 Training loss: 1.5907 Explore P: 0.9513\n",
    "Episode: 28 Total reward: 17.0 Training loss: 2.1144 Explore P: 0.9497\n",
    "Episode: 29 Total reward: 34.0 Training loss: 1.7340 Explore P: 0.9466\n",
    "Episode: 30 Total reward: 18.0 Training loss: 2.5100 Explore P: 0.9449\n",
    "Episode: 31 Total reward: 15.0 Training loss: 2.0166 Explore P: 0.9435\n",
    "Episode: 32 Total reward: 11.0 Training loss: 1.8675 Explore P: 0.9424\n",
    "Episode: 33 Total reward: 18.0 Training loss: 4.0481 Explore P: 0.9408\n",
    "Episode: 34 Total reward: 10.0 Training loss: 4.0895 Explore P: 0.9398\n",
    "Episode: 35 Total reward: 15.0 Training loss: 2.1252 Explore P: 0.9384\n",
    "Episode: 36 Total reward: 14.0 Training loss: 4.7765 Explore P: 0.9371\n",
    "Episode: 37 Total reward: 16.0 Training loss: 3.3848 Explore P: 0.9357\n",
    "Episode: 38 Total reward: 21.0 Training loss: 3.9125 Explore P: 0.9337\n",
    "Episode: 39 Total reward: 16.0 Training loss: 2.6183 Explore P: 0.9322\n",
    "Episode: 40 Total reward: 20.0 Training loss: 5.4929 Explore P: 0.9304\n",
    "Episode: 41 Total reward: 18.0 Training loss: 3.6606 Explore P: 0.9287\n",
    "Episode: 42 Total reward: 17.0 Training loss: 4.5812 Explore P: 0.9272\n",
    "Episode: 43 Total reward: 10.0 Training loss: 3.7633 Explore P: 0.9263\n",
    "Episode: 44 Total reward: 8.0 Training loss: 4.6176 Explore P: 0.9255\n",
    "Episode: 45 Total reward: 39.0 Training loss: 4.2732 Explore P: 0.9220\n",
    "Episode: 46 Total reward: 18.0 Training loss: 4.0041 Explore P: 0.9203\n",
    "Episode: 47 Total reward: 11.0 Training loss: 4.4035 Explore P: 0.9193\n",
    "Episode: 48 Total reward: 25.0 Training loss: 5.4287 Explore P: 0.9171\n",
    "Episode: 49 Total reward: 19.0 Training loss: 9.6972 Explore P: 0.9153\n",
    "Episode: 50 Total reward: 11.0 Training loss: 16.3460 Explore P: 0.9143\n",
    "Episode: 51 Total reward: 11.0 Training loss: 13.4854 Explore P: 0.9133\n",
    "Episode: 52 Total reward: 12.0 Training loss: 12.8016 Explore P: 0.9123\n",
    "Episode: 53 Total reward: 13.0 Training loss: 5.8589 Explore P: 0.9111\n",
    "Episode: 54 Total reward: 12.0 Training loss: 8.5924 Explore P: 0.9100\n",
    "Episode: 55 Total reward: 19.0 Training loss: 8.6204 Explore P: 0.9083\n",
    "Episode: 56 Total reward: 36.0 Training loss: 14.2701 Explore P: 0.9051\n",
    "Episode: 57 Total reward: 9.0 Training loss: 4.5481 Explore P: 0.9043\n",
    "Episode: 58 Total reward: 22.0 Training loss: 12.9695 Explore P: 0.9023\n",
    "Episode: 59 Total reward: 36.0 Training loss: 11.2639 Explore P: 0.8991\n",
    "Episode: 60 Total reward: 16.0 Training loss: 7.7648 Explore P: 0.8977\n",
    "Episode: 61 Total reward: 31.0 Training loss: 4.6997 Explore P: 0.8949\n",
    "Episode: 62 Total reward: 13.0 Training loss: 5.9755 Explore P: 0.8938\n",
    "Episode: 63 Total reward: 10.0 Training loss: 39.1040 Explore P: 0.8929\n",
    "Episode: 64 Total reward: 14.0 Training loss: 23.2767 Explore P: 0.8917\n",
    "Episode: 65 Total reward: 12.0 Training loss: 9.3477 Explore P: 0.8906\n",
    "Episode: 66 Total reward: 20.0 Training loss: 6.4336 Explore P: 0.8888\n",
    "Episode: 67 Total reward: 29.0 Training loss: 17.1522 Explore P: 0.8863\n",
    "Episode: 68 Total reward: 13.0 Training loss: 39.3250 Explore P: 0.8852\n",
    "Episode: 69 Total reward: 20.0 Training loss: 6.2099 Explore P: 0.8834\n",
    "Episode: 70 Total reward: 15.0 Training loss: 20.9229 Explore P: 0.8821\n",
    "Episode: 71 Total reward: 27.0 Training loss: 24.7817 Explore P: 0.8797\n",
    "Episode: 72 Total reward: 12.0 Training loss: 20.7842 Explore P: 0.8787\n",
    "Episode: 73 Total reward: 15.0 Training loss: 12.3202 Explore P: 0.8774\n",
    "Episode: 74 Total reward: 31.0 Training loss: 9.2270 Explore P: 0.8747\n",
    "Episode: 75 Total reward: 13.0 Training loss: 19.8264 Explore P: 0.8736\n",
    "Episode: 76 Total reward: 20.0 Training loss: 72.9411 Explore P: 0.8719\n",
    "Episode: 77 Total reward: 27.0 Training loss: 5.2214 Explore P: 0.8695\n",
    "Episode: 78 Total reward: 14.0 Training loss: 39.3913 Explore P: 0.8683\n",
    "Episode: 79 Total reward: 16.0 Training loss: 7.9491 Explore P: 0.8670\n",
    "Episode: 80 Total reward: 18.0 Training loss: 10.8364 Explore P: 0.8654\n",
    "Episode: 81 Total reward: 16.0 Training loss: 22.2031 Explore P: 0.8641\n",
    "Episode: 82 Total reward: 21.0 Training loss: 23.6590 Explore P: 0.8623\n",
    "Episode: 83 Total reward: 13.0 Training loss: 8.4819 Explore P: 0.8612\n",
    "Episode: 84 Total reward: 10.0 Training loss: 13.3548 Explore P: 0.8603\n",
    "Episode: 85 Total reward: 13.0 Training loss: 18.0272 Explore P: 0.8592\n",
    "Episode: 86 Total reward: 24.0 Training loss: 42.1243 Explore P: 0.8572\n",
    "Episode: 87 Total reward: 9.0 Training loss: 30.8526 Explore P: 0.8564\n",
    "Episode: 88 Total reward: 22.0 Training loss: 36.6084 Explore P: 0.8546\n",
    "Episode: 89 Total reward: 7.0 Training loss: 10.5430 Explore P: 0.8540\n",
    "Episode: 90 Total reward: 12.0 Training loss: 25.5808 Explore P: 0.8529\n",
    "Episode: 91 Total reward: 17.0 Training loss: 47.3073 Explore P: 0.8515\n",
    "Episode: 92 Total reward: 21.0 Training loss: 7.9998 Explore P: 0.8498\n",
    "Episode: 93 Total reward: 15.0 Training loss: 66.6464 Explore P: 0.8485\n",
    "Episode: 94 Total reward: 17.0 Training loss: 95.6354 Explore P: 0.8471\n",
    "Episode: 95 Total reward: 23.0 Training loss: 57.4714 Explore P: 0.8451\n",
    "Episode: 96 Total reward: 11.0 Training loss: 40.7717 Explore P: 0.8442\n",
    "Episode: 97 Total reward: 13.0 Training loss: 43.3380 Explore P: 0.8431\n",
    "Episode: 98 Total reward: 9.0 Training loss: 10.8368 Explore P: 0.8424\n",
    "Episode: 99 Total reward: 21.0 Training loss: 57.7325 Explore P: 0.8406\n",
    "Episode: 100 Total reward: 11.0 Training loss: 9.7291 Explore P: 0.8397\n",
    "Episode: 101 Total reward: 10.0 Training loss: 10.4052 Explore P: 0.8389\n",
    "Episode: 102 Total reward: 26.0 Training loss: 60.4829 Explore P: 0.8368\n",
    "Episode: 103 Total reward: 34.0 Training loss: 9.0924 Explore P: 0.8339\n",
    "Episode: 104 Total reward: 30.0 Training loss: 178.0664 Explore P: 0.8315\n",
    "Episode: 105 Total reward: 14.0 Training loss: 9.0423 Explore P: 0.8303\n",
    "Episode: 106 Total reward: 18.0 Training loss: 126.9380 Explore P: 0.8289\n",
    "Episode: 107 Total reward: 11.0 Training loss: 58.9921 Explore P: 0.8280\n",
    "Episode: 108 Total reward: 20.0 Training loss: 9.1945 Explore P: 0.8263\n",
    "Episode: 109 Total reward: 9.0 Training loss: 9.2887 Explore P: 0.8256\n",
    "Episode: 110 Total reward: 29.0 Training loss: 20.7970 Explore P: 0.8232\n",
    "Episode: 111 Total reward: 17.0 Training loss: 144.6258 Explore P: 0.8218\n",
    "Episode: 112 Total reward: 15.0 Training loss: 82.4089 Explore P: 0.8206\n",
    "Episode: 113 Total reward: 15.0 Training loss: 39.9963 Explore P: 0.8194\n",
    "Episode: 114 Total reward: 8.0 Training loss: 9.8394 Explore P: 0.8188\n",
    "Episode: 115 Total reward: 29.0 Training loss: 76.9930 Explore P: 0.8164\n",
    "Episode: 116 Total reward: 21.0 Training loss: 25.0172 Explore P: 0.8147\n",
    "Episode: 117 Total reward: 24.0 Training loss: 143.5481 Explore P: 0.8128\n",
    "Episode: 118 Total reward: 35.0 Training loss: 86.5429 Explore P: 0.8100\n",
    "Episode: 119 Total reward: 28.0 Training loss: 8.4315 Explore P: 0.8078\n",
    "Episode: 120 Total reward: 13.0 Training loss: 25.7062 Explore P: 0.8067\n",
    "Episode: 121 Total reward: 9.0 Training loss: 6.5005 Explore P: 0.8060\n",
    "Episode: 122 Total reward: 32.0 Training loss: 90.7984 Explore P: 0.8035\n",
    "Episode: 123 Total reward: 21.0 Training loss: 130.2779 Explore P: 0.8018\n",
    "Episode: 124 Total reward: 15.0 Training loss: 167.6294 Explore P: 0.8006\n",
    "Episode: 125 Total reward: 15.0 Training loss: 74.7611 Explore P: 0.7994\n",
    "Episode: 126 Total reward: 20.0 Training loss: 119.3178 Explore P: 0.7978\n",
    "Episode: 127 Total reward: 18.0 Training loss: 196.5175 Explore P: 0.7964\n",
    "Episode: 128 Total reward: 8.0 Training loss: 45.2131 Explore P: 0.7958\n",
    "Episode: 129 Total reward: 24.0 Training loss: 86.0374 Explore P: 0.7939\n",
    "Episode: 130 Total reward: 11.0 Training loss: 7.8129 Explore P: 0.7931\n",
    "Episode: 131 Total reward: 11.0 Training loss: 76.8442 Explore P: 0.7922\n",
    "Episode: 132 Total reward: 28.0 Training loss: 196.6863 Explore P: 0.7900\n",
    "Episode: 133 Total reward: 9.0 Training loss: 45.7586 Explore P: 0.7893\n",
    "Episode: 134 Total reward: 21.0 Training loss: 5.8484 Explore P: 0.7877\n",
    "Episode: 135 Total reward: 10.0 Training loss: 7.3919 Explore P: 0.7869\n",
    "Episode: 136 Total reward: 17.0 Training loss: 12.3142 Explore P: 0.7856\n",
    "Episode: 137 Total reward: 16.0 Training loss: 75.7170 Explore P: 0.7843\n",
    "Episode: 138 Total reward: 12.0 Training loss: 145.3568 Explore P: 0.7834\n",
    "Episode: 139 Total reward: 27.0 Training loss: 121.1114 Explore P: 0.7813\n",
    "Episode: 140 Total reward: 26.0 Training loss: 7.3243 Explore P: 0.7793\n",
    "Episode: 141 Total reward: 40.0 Training loss: 10.6523 Explore P: 0.7762\n",
    "Episode: 142 Total reward: 14.0 Training loss: 6.9482 Explore P: 0.7752\n",
    "Episode: 143 Total reward: 24.0 Training loss: 137.7784 Explore P: 0.7733\n",
    "Episode: 144 Total reward: 12.0 Training loss: 98.8381 Explore P: 0.7724\n",
    "Episode: 145 Total reward: 8.0 Training loss: 14.1739 Explore P: 0.7718\n",
    "Episode: 146 Total reward: 51.0 Training loss: 69.1545 Explore P: 0.7679\n",
    "Episode: 147 Total reward: 29.0 Training loss: 249.9989 Explore P: 0.7657\n",
    "Episode: 148 Total reward: 9.0 Training loss: 140.8663 Explore P: 0.7651\n",
    "Episode: 149 Total reward: 13.0 Training loss: 141.5930 Explore P: 0.7641\n",
    "Episode: 150 Total reward: 19.0 Training loss: 12.6228 Explore P: 0.7627\n",
    "Episode: 151 Total reward: 19.0 Training loss: 136.3315 Explore P: 0.7612\n",
    "Episode: 152 Total reward: 10.0 Training loss: 110.3699 Explore P: 0.7605\n",
    "Episode: 153 Total reward: 18.0 Training loss: 8.3900 Explore P: 0.7591\n",
    "Episode: 154 Total reward: 18.0 Training loss: 96.3717 Explore P: 0.7578\n",
    "Episode: 155 Total reward: 7.0 Training loss: 6.0889 Explore P: 0.7573\n",
    "Episode: 156 Total reward: 15.0 Training loss: 126.7419 Explore P: 0.7561\n",
    "Episode: 157 Total reward: 15.0 Training loss: 67.2544 Explore P: 0.7550\n",
    "Episode: 158 Total reward: 26.0 Training loss: 12.2839 Explore P: 0.7531\n",
    "Episode: 159 Total reward: 20.0 Training loss: 5.8118 Explore P: 0.7516\n",
    "Episode: 160 Total reward: 10.0 Training loss: 96.4570 Explore P: 0.7509\n",
    "Episode: 161 Total reward: 23.0 Training loss: 7.6207 Explore P: 0.7492\n",
    "Episode: 162 Total reward: 18.0 Training loss: 66.5249 Explore P: 0.7478\n",
    "Episode: 163 Total reward: 18.0 Training loss: 111.3273 Explore P: 0.7465\n",
    "Episode: 164 Total reward: 21.0 Training loss: 11.5292 Explore P: 0.7450\n",
    "Episode: 165 Total reward: 17.0 Training loss: 6.3130 Explore P: 0.7437\n",
    "Episode: 166 Total reward: 22.0 Training loss: 153.8167 Explore P: 0.7421\n",
    "Episode: 167 Total reward: 17.0 Training loss: 7.0915 Explore P: 0.7408\n",
    "Episode: 168 Total reward: 34.0 Training loss: 228.3831 Explore P: 0.7384\n",
    "Episode: 169 Total reward: 13.0 Training loss: 8.5996 Explore P: 0.7374\n",
    "Episode: 170 Total reward: 13.0 Training loss: 90.8898 Explore P: 0.7365\n",
    "Episode: 171 Total reward: 20.0 Training loss: 4.8179 Explore P: 0.7350\n",
    "Episode: 172 Total reward: 9.0 Training loss: 6.2508 Explore P: 0.7344\n",
    "Episode: 173 Total reward: 14.0 Training loss: 5.2401 Explore P: 0.7334\n",
    "Episode: 174 Total reward: 12.0 Training loss: 3.9268 Explore P: 0.7325\n",
    "Episode: 175 Total reward: 12.0 Training loss: 5.6376 Explore P: 0.7316\n",
    "Episode: 176 Total reward: 11.0 Training loss: 44.5308 Explore P: 0.7308\n",
    "Episode: 177 Total reward: 12.0 Training loss: 4.9717 Explore P: 0.7300\n",
    "Episode: 178 Total reward: 9.0 Training loss: 181.0085 Explore P: 0.7293\n",
    "Episode: 179 Total reward: 11.0 Training loss: 73.1134 Explore P: 0.7285\n",
    "Episode: 180 Total reward: 13.0 Training loss: 87.3085 Explore P: 0.7276\n",
    "Episode: 181 Total reward: 12.0 Training loss: 121.6627 Explore P: 0.7267\n",
    "Episode: 182 Total reward: 12.0 Training loss: 58.1967 Explore P: 0.7259\n",
    "Episode: 183 Total reward: 13.0 Training loss: 85.1540 Explore P: 0.7249\n",
    "Episode: 184 Total reward: 16.0 Training loss: 5.1214 Explore P: 0.7238\n",
    "Episode: 185 Total reward: 19.0 Training loss: 69.1839 Explore P: 0.7224\n",
    "Episode: 186 Total reward: 7.0 Training loss: 63.2256 Explore P: 0.7219\n",
    "Episode: 187 Total reward: 17.0 Training loss: 73.2788 Explore P: 0.7207\n",
    "Episode: 188 Total reward: 15.0 Training loss: 78.6213 Explore P: 0.7197\n",
    "Episode: 189 Total reward: 11.0 Training loss: 88.5211 Explore P: 0.7189\n",
    "Episode: 190 Total reward: 14.0 Training loss: 60.1332 Explore P: 0.7179\n",
    "Episode: 191 Total reward: 15.0 Training loss: 135.7724 Explore P: 0.7168\n",
    "Episode: 192 Total reward: 15.0 Training loss: 156.9691 Explore P: 0.7158\n",
    "Episode: 193 Total reward: 17.0 Training loss: 93.3756 Explore P: 0.7146\n",
    "Episode: 194 Total reward: 12.0 Training loss: 3.0462 Explore P: 0.7137\n",
    "Episode: 195 Total reward: 9.0 Training loss: 119.2650 Explore P: 0.7131\n",
    "Episode: 196 Total reward: 13.0 Training loss: 66.6383 Explore P: 0.7122\n",
    "Episode: 197 Total reward: 9.0 Training loss: 113.7849 Explore P: 0.7116\n",
    "Episode: 198 Total reward: 13.0 Training loss: 54.6072 Explore P: 0.7106\n",
    "Episode: 199 Total reward: 19.0 Training loss: 54.8980 Explore P: 0.7093\n",
    "Episode: 200 Total reward: 20.0 Training loss: 155.5480 Explore P: 0.7079\n",
    "Episode: 201 Total reward: 10.0 Training loss: 45.8685 Explore P: 0.7072\n",
    "Episode: 202 Total reward: 14.0 Training loss: 53.5145 Explore P: 0.7062\n",
    "Episode: 203 Total reward: 8.0 Training loss: 107.9623 Explore P: 0.7057\n",
    "Episode: 204 Total reward: 21.0 Training loss: 40.2749 Explore P: 0.7042\n",
    "Episode: 205 Total reward: 32.0 Training loss: 43.2627 Explore P: 0.7020\n",
    "Episode: 206 Total reward: 9.0 Training loss: 55.5398 Explore P: 0.7014\n",
    "Episode: 207 Total reward: 15.0 Training loss: 1.9959 Explore P: 0.7004\n",
    "Episode: 208 Total reward: 12.0 Training loss: 105.3751 Explore P: 0.6995\n",
    "Episode: 209 Total reward: 11.0 Training loss: 40.8319 Explore P: 0.6988\n",
    "Episode: 210 Total reward: 10.0 Training loss: 89.7147 Explore P: 0.6981\n",
    "Episode: 211 Total reward: 10.0 Training loss: 1.1946 Explore P: 0.6974\n",
    "Episode: 212 Total reward: 10.0 Training loss: 80.6916 Explore P: 0.6967\n",
    "Episode: 213 Total reward: 24.0 Training loss: 88.0977 Explore P: 0.6951\n",
    "Episode: 214 Total reward: 14.0 Training loss: 46.4105 Explore P: 0.6941\n",
    "Episode: 215 Total reward: 11.0 Training loss: 40.3726 Explore P: 0.6933\n",
    "Episode: 216 Total reward: 11.0 Training loss: 3.0770 Explore P: 0.6926\n",
    "Episode: 217 Total reward: 8.0 Training loss: 1.7495 Explore P: 0.6921\n",
    "Episode: 218 Total reward: 16.0 Training loss: 1.5615 Explore P: 0.6910\n",
    "Episode: 219 Total reward: 17.0 Training loss: 2.0250 Explore P: 0.6898\n",
    "Episode: 220 Total reward: 18.0 Training loss: 37.8432 Explore P: 0.6886\n",
    "Episode: 221 Total reward: 17.0 Training loss: 1.9049 Explore P: 0.6874\n",
    "Episode: 222 Total reward: 16.0 Training loss: 1.9652 Explore P: 0.6863\n",
    "Episode: 223 Total reward: 16.0 Training loss: 1.4384 Explore P: 0.6853\n",
    "Episode: 224 Total reward: 27.0 Training loss: 66.0615 Explore P: 0.6834\n",
    "Episode: 225 Total reward: 9.0 Training loss: 0.8478 Explore P: 0.6828\n",
    "Episode: 226 Total reward: 14.0 Training loss: 1.0319 Explore P: 0.6819\n",
    "Episode: 227 Total reward: 17.0 Training loss: 97.6957 Explore P: 0.6808\n",
    "Episode: 228 Total reward: 8.0 Training loss: 68.0521 Explore P: 0.6802\n",
    "Episode: 229 Total reward: 9.0 Training loss: 110.8437 Explore P: 0.6796\n",
    "Episode: 230 Total reward: 19.0 Training loss: 1.6856 Explore P: 0.6783\n",
    "Episode: 231 Total reward: 9.0 Training loss: 2.0634 Explore P: 0.6777\n",
    "Episode: 232 Total reward: 11.0 Training loss: 32.0714 Explore P: 0.6770\n",
    "Episode: 233 Total reward: 9.0 Training loss: 2.0387 Explore P: 0.6764\n",
    "Episode: 234 Total reward: 13.0 Training loss: 66.9349 Explore P: 0.6755\n",
    "Episode: 235 Total reward: 14.0 Training loss: 110.6725 Explore P: 0.6746\n",
    "Episode: 236 Total reward: 18.0 Training loss: 1.0585 Explore P: 0.6734\n",
    "Episode: 237 Total reward: 11.0 Training loss: 117.0101 Explore P: 0.6727\n",
    "Episode: 238 Total reward: 7.0 Training loss: 2.6115 Explore P: 0.6722\n",
    "Episode: 239 Total reward: 10.0 Training loss: 124.7320 Explore P: 0.6716\n",
    "Episode: 240 Total reward: 18.0 Training loss: 2.5475 Explore P: 0.6704\n",
    "Episode: 241 Total reward: 37.0 Training loss: 2.1454 Explore P: 0.6679\n",
    "Episode: 242 Total reward: 11.0 Training loss: 23.6042 Explore P: 0.6672\n",
    "Episode: 243 Total reward: 32.0 Training loss: 1.4344 Explore P: 0.6651\n",
    "Episode: 244 Total reward: 9.0 Training loss: 1.5328 Explore P: 0.6645\n",
    "Episode: 245 Total reward: 14.0 Training loss: 84.7870 Explore P: 0.6636\n",
    "Episode: 246 Total reward: 12.0 Training loss: 2.7292 Explore P: 0.6628\n",
    "Episode: 247 Total reward: 26.0 Training loss: 40.6692 Explore P: 0.6611\n",
    "Episode: 248 Total reward: 12.0 Training loss: 22.0901 Explore P: 0.6603\n",
    "Episode: 249 Total reward: 15.0 Training loss: 37.9304 Explore P: 0.6594\n",
    "Episode: 250 Total reward: 20.0 Training loss: 1.4137 Explore P: 0.6581\n",
    "Episode: 251 Total reward: 16.0 Training loss: 1.7831 Explore P: 0.6570\n",
    "Episode: 252 Total reward: 9.0 Training loss: 38.0640 Explore P: 0.6565\n",
    "Episode: 253 Total reward: 17.0 Training loss: 21.7703 Explore P: 0.6554\n",
    "Episode: 254 Total reward: 24.0 Training loss: 40.3204 Explore P: 0.6538\n",
    "Episode: 255 Total reward: 30.0 Training loss: 43.4179 Explore P: 0.6519\n",
    "Episode: 256 Total reward: 11.0 Training loss: 60.9330 Explore P: 0.6512\n",
    "Episode: 257 Total reward: 14.0 Training loss: 66.6886 Explore P: 0.6503\n",
    "Episode: 258 Total reward: 15.0 Training loss: 2.5639 Explore P: 0.6493\n",
    "Episode: 259 Total reward: 19.0 Training loss: 2.6969 Explore P: 0.6481\n",
    "Episode: 260 Total reward: 10.0 Training loss: 2.6837 Explore P: 0.6475\n",
    "Episode: 261 Total reward: 30.0 Training loss: 20.6603 Explore P: 0.6456\n",
    "Episode: 262 Total reward: 17.0 Training loss: 32.1585 Explore P: 0.6445\n",
    "Episode: 263 Total reward: 15.0 Training loss: 1.0833 Explore P: 0.6435\n",
    "Episode: 264 Total reward: 13.0 Training loss: 81.4551 Explore P: 0.6427\n",
    "Episode: 265 Total reward: 17.0 Training loss: 3.3823 Explore P: 0.6416\n",
    "Episode: 266 Total reward: 11.0 Training loss: 36.3942 Explore P: 0.6409\n",
    "Episode: 267 Total reward: 11.0 Training loss: 1.6628 Explore P: 0.6402\n",
    "Episode: 268 Total reward: 33.0 Training loss: 26.9925 Explore P: 0.6382\n",
    "Episode: 269 Total reward: 18.0 Training loss: 45.8608 Explore P: 0.6370\n",
    "Episode: 270 Total reward: 20.0 Training loss: 2.7911 Explore P: 0.6358\n",
    "Episode: 271 Total reward: 10.0 Training loss: 35.9215 Explore P: 0.6352\n",
    "Episode: 272 Total reward: 14.0 Training loss: 2.5923 Explore P: 0.6343\n",
    "Episode: 273 Total reward: 16.0 Training loss: 41.2339 Explore P: 0.6333\n",
    "Episode: 274 Total reward: 18.0 Training loss: 46.7318 Explore P: 0.6322\n",
    "Episode: 275 Total reward: 14.0 Training loss: 2.7245 Explore P: 0.6313\n",
    "Episode: 276 Total reward: 8.0 Training loss: 16.2681 Explore P: 0.6308\n",
    "Episode: 277 Total reward: 10.0 Training loss: 21.6856 Explore P: 0.6302\n",
    "Episode: 278 Total reward: 12.0 Training loss: 1.7879 Explore P: 0.6294\n",
    "Episode: 279 Total reward: 10.0 Training loss: 97.1567 Explore P: 0.6288\n",
    "Episode: 280 Total reward: 16.0 Training loss: 3.4710 Explore P: 0.6278\n",
    "Episode: 281 Total reward: 14.0 Training loss: 65.8457 Explore P: 0.6270\n",
    "Episode: 282 Total reward: 21.0 Training loss: 32.4442 Explore P: 0.6257\n",
    "Episode: 283 Total reward: 17.0 Training loss: 48.0136 Explore P: 0.6246\n",
    "Episode: 284 Total reward: 11.0 Training loss: 2.8833 Explore P: 0.6239\n",
    "Episode: 285 Total reward: 16.0 Training loss: 92.6062 Explore P: 0.6230\n",
    "Episode: 286 Total reward: 16.0 Training loss: 19.1051 Explore P: 0.6220\n",
    "Episode: 287 Total reward: 7.0 Training loss: 1.8220 Explore P: 0.6216\n",
    "Episode: 288 Total reward: 16.0 Training loss: 41.3844 Explore P: 0.6206\n",
    "Episode: 289 Total reward: 18.0 Training loss: 50.0580 Explore P: 0.6195\n",
    "Episode: 290 Total reward: 13.0 Training loss: 83.2142 Explore P: 0.6187\n",
    "Episode: 291 Total reward: 14.0 Training loss: 70.2605 Explore P: 0.6178\n",
    "Episode: 292 Total reward: 16.0 Training loss: 53.9664 Explore P: 0.6169\n",
    "Episode: 293 Total reward: 17.0 Training loss: 3.2764 Explore P: 0.6158\n",
    "Episode: 294 Total reward: 18.0 Training loss: 17.7963 Explore P: 0.6147\n",
    "Episode: 295 Total reward: 17.0 Training loss: 32.3772 Explore P: 0.6137\n",
    "Episode: 296 Total reward: 32.0 Training loss: 18.3755 Explore P: 0.6118\n",
    "Episode: 297 Total reward: 29.0 Training loss: 17.1377 Explore P: 0.6100\n",
    "Episode: 298 Total reward: 12.0 Training loss: 14.2922 Explore P: 0.6093\n",
    "Episode: 299 Total reward: 14.0 Training loss: 29.2226 Explore P: 0.6085\n",
    "Episode: 300 Total reward: 17.0 Training loss: 38.9089 Explore P: 0.6075\n",
    "Episode: 301 Total reward: 9.0 Training loss: 62.2483 Explore P: 0.6069\n",
    "Episode: 302 Total reward: 22.0 Training loss: 2.3240 Explore P: 0.6056\n",
    "Episode: 303 Total reward: 16.0 Training loss: 0.9979 Explore P: 0.6047\n",
    "Episode: 304 Total reward: 8.0 Training loss: 67.9231 Explore P: 0.6042\n",
    "Episode: 305 Total reward: 13.0 Training loss: 33.0928 Explore P: 0.6034\n",
    "Episode: 306 Total reward: 20.0 Training loss: 1.3173 Explore P: 0.6022\n",
    "Episode: 307 Total reward: 23.0 Training loss: 50.2106 Explore P: 0.6009\n",
    "Episode: 308 Total reward: 17.0 Training loss: 52.5245 Explore P: 0.5999\n",
    "Episode: 309 Total reward: 20.0 Training loss: 32.5832 Explore P: 0.5987\n",
    "Episode: 310 Total reward: 19.0 Training loss: 29.0224 Explore P: 0.5976\n",
    "Episode: 311 Total reward: 19.0 Training loss: 29.8863 Explore P: 0.5965\n",
    "Episode: 312 Total reward: 27.0 Training loss: 34.4016 Explore P: 0.5949\n",
    "Episode: 313 Total reward: 9.0 Training loss: 1.1433 Explore P: 0.5944\n",
    "Episode: 314 Total reward: 20.0 Training loss: 28.8137 Explore P: 0.5932\n",
    "Episode: 315 Total reward: 24.0 Training loss: 48.5379 Explore P: 0.5918\n",
    "Episode: 316 Total reward: 28.0 Training loss: 45.2671 Explore P: 0.5902\n",
    "Episode: 317 Total reward: 13.0 Training loss: 45.9822 Explore P: 0.5894\n",
    "Episode: 318 Total reward: 12.0 Training loss: 86.3972 Explore P: 0.5887\n",
    "Episode: 319 Total reward: 10.0 Training loss: 11.2909 Explore P: 0.5881\n",
    "Episode: 320 Total reward: 11.0 Training loss: 36.5474 Explore P: 0.5875\n",
    "Episode: 321 Total reward: 13.0 Training loss: 1.1439 Explore P: 0.5867\n",
    "Episode: 322 Total reward: 8.0 Training loss: 12.6978 Explore P: 0.5863\n",
    "Episode: 323 Total reward: 20.0 Training loss: 31.7664 Explore P: 0.5851\n",
    "Episode: 324 Total reward: 8.0 Training loss: 29.4243 Explore P: 0.5847\n",
    "Episode: 325 Total reward: 13.0 Training loss: 12.2373 Explore P: 0.5839\n",
    "Episode: 326 Total reward: 19.0 Training loss: 24.2228 Explore P: 0.5828\n",
    "Episode: 327 Total reward: 68.0 Training loss: 0.7256 Explore P: 0.5790\n",
    "Episode: 328 Total reward: 11.0 Training loss: 1.2313 Explore P: 0.5783\n",
    "Episode: 329 Total reward: 15.0 Training loss: 1.3319 Explore P: 0.5775\n",
    "Episode: 330 Total reward: 53.0 Training loss: 9.9350 Explore P: 0.5745\n",
    "Episode: 331 Total reward: 74.0 Training loss: 1.4366 Explore P: 0.5703\n",
    "Episode: 332 Total reward: 16.0 Training loss: 11.2724 Explore P: 0.5694\n",
    "Episode: 333 Total reward: 34.0 Training loss: 10.6128 Explore P: 0.5675\n",
    "Episode: 334 Total reward: 27.0 Training loss: 14.9559 Explore P: 0.5660\n",
    "Episode: 335 Total reward: 31.0 Training loss: 16.6541 Explore P: 0.5643\n",
    "Episode: 336 Total reward: 49.0 Training loss: 23.3966 Explore P: 0.5616\n",
    "Episode: 337 Total reward: 40.0 Training loss: 45.3419 Explore P: 0.5594\n",
    "Episode: 338 Total reward: 71.0 Training loss: 0.8244 Explore P: 0.5555\n",
    "Episode: 339 Total reward: 56.0 Training loss: 41.4562 Explore P: 0.5525\n",
    "Episode: 340 Total reward: 18.0 Training loss: 1.2548 Explore P: 0.5515\n",
    "Episode: 341 Total reward: 56.0 Training loss: 1.5400 Explore P: 0.5485\n",
    "Episode: 342 Total reward: 34.0 Training loss: 12.0206 Explore P: 0.5466\n",
    "Episode: 343 Total reward: 67.0 Training loss: 1.4189 Explore P: 0.5430\n",
    "Episode: 344 Total reward: 27.0 Training loss: 1.3138 Explore P: 0.5416\n",
    "Episode: 345 Total reward: 42.0 Training loss: 1.1650 Explore P: 0.5394\n",
    "Episode: 346 Total reward: 23.0 Training loss: 23.1743 Explore P: 0.5382\n",
    "Episode: 347 Total reward: 54.0 Training loss: 0.6971 Explore P: 0.5353\n",
    "Episode: 348 Total reward: 34.0 Training loss: 27.2789 Explore P: 0.5335\n",
    "Episode: 349 Total reward: 25.0 Training loss: 37.4133 Explore P: 0.5322\n",
    "Episode: 350 Total reward: 20.0 Training loss: 1.6443 Explore P: 0.5312\n",
    "Episode: 351 Total reward: 26.0 Training loss: 12.6839 Explore P: 0.5298\n",
    "Episode: 352 Total reward: 40.0 Training loss: 13.3593 Explore P: 0.5278\n",
    "Episode: 353 Total reward: 18.0 Training loss: 1.7079 Explore P: 0.5268\n",
    "Episode: 354 Total reward: 47.0 Training loss: 32.5788 Explore P: 0.5244\n",
    "Episode: 355 Total reward: 20.0 Training loss: 1.6101 Explore P: 0.5234\n",
    "Episode: 356 Total reward: 53.0 Training loss: 2.5321 Explore P: 0.5207\n",
    "Episode: 357 Total reward: 15.0 Training loss: 1.6396 Explore P: 0.5199\n",
    "Episode: 358 Total reward: 76.0 Training loss: 20.8058 Explore P: 0.5160\n",
    "Episode: 359 Total reward: 12.0 Training loss: 13.0315 Explore P: 0.5154\n",
    "Episode: 360 Total reward: 42.0 Training loss: 10.1313 Explore P: 0.5133\n",
    "Episode: 361 Total reward: 53.0 Training loss: 25.4319 Explore P: 0.5106\n",
    "Episode: 362 Total reward: 33.0 Training loss: 26.4256 Explore P: 0.5090\n",
    "Episode: 363 Total reward: 85.0 Training loss: 20.2429 Explore P: 0.5048\n",
    "Episode: 364 Total reward: 23.0 Training loss: 16.1083 Explore P: 0.5036\n",
    "Episode: 365 Total reward: 30.0 Training loss: 1.6888 Explore P: 0.5022\n",
    "Episode: 366 Total reward: 66.0 Training loss: 2.0408 Explore P: 0.4989\n",
    "Episode: 367 Total reward: 37.0 Training loss: 18.6438 Explore P: 0.4971\n",
    "Episode: 368 Total reward: 50.0 Training loss: 20.1544 Explore P: 0.4947\n",
    "Episode: 369 Total reward: 78.0 Training loss: 23.8497 Explore P: 0.4909\n",
    "Episode: 370 Total reward: 83.0 Training loss: 20.6897 Explore P: 0.4869\n",
    "Episode: 371 Total reward: 44.0 Training loss: 25.4317 Explore P: 0.4849\n",
    "Episode: 372 Total reward: 44.0 Training loss: 1.5212 Explore P: 0.4828\n",
    "Episode: 373 Total reward: 14.0 Training loss: 1.5019 Explore P: 0.4821\n",
    "Episode: 374 Total reward: 31.0 Training loss: 1.8348 Explore P: 0.4806\n",
    "Episode: 375 Total reward: 25.0 Training loss: 19.7533 Explore P: 0.4795\n",
    "Episode: 376 Total reward: 51.0 Training loss: 1.5433 Explore P: 0.4771\n",
    "Episode: 377 Total reward: 23.0 Training loss: 12.9174 Explore P: 0.4760\n",
    "Episode: 378 Total reward: 67.0 Training loss: 27.2318 Explore P: 0.4729\n",
    "Episode: 379 Total reward: 26.0 Training loss: 1.9319 Explore P: 0.4717\n",
    "Episode: 380 Total reward: 35.0 Training loss: 43.2445 Explore P: 0.4701\n",
    "Episode: 381 Total reward: 33.0 Training loss: 1.5195 Explore P: 0.4686\n",
    "Episode: 382 Total reward: 30.0 Training loss: 15.4622 Explore P: 0.4672\n",
    "Episode: 383 Total reward: 12.0 Training loss: 1.8349 Explore P: 0.4666\n",
    "Episode: 384 Total reward: 25.0 Training loss: 47.7600 Explore P: 0.4655\n",
    "Episode: 385 Total reward: 36.0 Training loss: 29.6753 Explore P: 0.4639\n",
    "Episode: 386 Total reward: 50.0 Training loss: 1.1244 Explore P: 0.4616\n",
    "Episode: 387 Total reward: 35.0 Training loss: 1.0955 Explore P: 0.4600\n",
    "Episode: 388 Total reward: 52.0 Training loss: 24.9624 Explore P: 0.4577\n",
    "Episode: 389 Total reward: 52.0 Training loss: 28.2028 Explore P: 0.4554\n",
    "Episode: 390 Total reward: 132.0 Training loss: 30.5190 Explore P: 0.4495\n",
    "Episode: 391 Total reward: 25.0 Training loss: 10.3908 Explore P: 0.4484\n",
    "Episode: 392 Total reward: 56.0 Training loss: 14.1483 Explore P: 0.4460\n",
    "Episode: 393 Total reward: 110.0 Training loss: 2.0169 Explore P: 0.4412\n",
    "Episode: 394 Total reward: 78.0 Training loss: 1.2122 Explore P: 0.4379\n",
    "Episode: 395 Total reward: 44.0 Training loss: 56.4728 Explore P: 0.4360\n",
    "Episode: 396 Total reward: 90.0 Training loss: 65.6667 Explore P: 0.4322\n",
    "Episode: 397 Total reward: 36.0 Training loss: 0.9032 Explore P: 0.4307\n",
    "Episode: 398 Total reward: 40.0 Training loss: 0.8414 Explore P: 0.4290\n",
    "Episode: 399 Total reward: 109.0 Training loss: 42.5467 Explore P: 0.4244\n",
    "Episode: 400 Total reward: 37.0 Training loss: 2.6053 Explore P: 0.4229\n",
    "Episode: 401 Total reward: 62.0 Training loss: 1.2301 Explore P: 0.4203\n",
    "Episode: 402 Total reward: 42.0 Training loss: 1.1384 Explore P: 0.4186\n",
    "Episode: 403 Total reward: 71.0 Training loss: 1.6765 Explore P: 0.4157\n",
    "Episode: 404 Total reward: 88.0 Training loss: 2.4000 Explore P: 0.4122\n",
    "Episode: 405 Total reward: 55.0 Training loss: 24.7748 Explore P: 0.4100\n",
    "Episode: 406 Total reward: 33.0 Training loss: 13.5934 Explore P: 0.4087\n",
    "Episode: 407 Total reward: 44.0 Training loss: 14.6865 Explore P: 0.4069\n",
    "Episode: 408 Total reward: 40.0 Training loss: 2.0898 Explore P: 0.4053\n",
    "Episode: 409 Total reward: 98.0 Training loss: 2.1043 Explore P: 0.4015\n",
    "Episode: 410 Total reward: 63.0 Training loss: 11.3562 Explore P: 0.3990\n",
    "Episode: 411 Total reward: 50.0 Training loss: 14.1151 Explore P: 0.3971\n",
    "Episode: 412 Total reward: 44.0 Training loss: 1.2370 Explore P: 0.3954\n",
    "Episode: 413 Total reward: 56.0 Training loss: 2.1136 Explore P: 0.3932\n",
    "Episode: 414 Total reward: 61.0 Training loss: 2.2578 Explore P: 0.3909\n",
    "Episode: 415 Total reward: 49.0 Training loss: 1.3966 Explore P: 0.3890\n",
    "Episode: 416 Total reward: 55.0 Training loss: 10.2836 Explore P: 0.3869\n",
    "Episode: 417 Total reward: 121.0 Training loss: 2.2477 Explore P: 0.3824\n",
    "Episode: 418 Total reward: 46.0 Training loss: 2.3118 Explore P: 0.3807\n",
    "Episode: 419 Total reward: 70.0 Training loss: 27.3952 Explore P: 0.3781\n",
    "Episode: 420 Total reward: 72.0 Training loss: 45.7570 Explore P: 0.3755\n",
    "Episode: 421 Total reward: 41.0 Training loss: 59.1887 Explore P: 0.3740\n",
    "Episode: 422 Total reward: 67.0 Training loss: 27.0998 Explore P: 0.3716\n",
    "Episode: 423 Total reward: 46.0 Training loss: 43.1971 Explore P: 0.3699\n",
    "Episode: 424 Total reward: 52.0 Training loss: 2.0718 Explore P: 0.3680\n",
    "Episode: 425 Total reward: 92.0 Training loss: 96.7074 Explore P: 0.3647\n",
    "Episode: 426 Total reward: 60.0 Training loss: 2.0684 Explore P: 0.3626\n",
    "Episode: 427 Total reward: 106.0 Training loss: 54.1831 Explore P: 0.3589\n",
    "Episode: 428 Total reward: 76.0 Training loss: 1.9612 Explore P: 0.3563\n",
    "Episode: 429 Total reward: 42.0 Training loss: 1.6153 Explore P: 0.3548\n",
    "Episode: 430 Total reward: 77.0 Training loss: 3.9801 Explore P: 0.3522\n",
    "Episode: 431 Total reward: 123.0 Training loss: 2.0505 Explore P: 0.3480\n",
    "Episode: 432 Total reward: 150.0 Training loss: 19.4217 Explore P: 0.3430\n",
    "Episode: 433 Total reward: 57.0 Training loss: 1.5850 Explore P: 0.3411\n",
    "Episode: 434 Total reward: 74.0 Training loss: 2.4292 Explore P: 0.3386\n",
    "Episode: 435 Total reward: 97.0 Training loss: 23.5709 Explore P: 0.3354\n",
    "Episode: 436 Total reward: 99.0 Training loss: 2.0727 Explore P: 0.3322\n",
    "Episode: 437 Total reward: 101.0 Training loss: 22.3250 Explore P: 0.3290\n",
    "Episode: 438 Total reward: 46.0 Training loss: 2.0320 Explore P: 0.3275\n",
    "Episode: 439 Total reward: 51.0 Training loss: 4.8099 Explore P: 0.3259\n",
    "Episode: 440 Total reward: 111.0 Training loss: 68.3524 Explore P: 0.3224\n",
    "Episode: 441 Total reward: 167.0 Training loss: 2.3045 Explore P: 0.3173\n",
    "Episode: 442 Total reward: 80.0 Training loss: 0.8798 Explore P: 0.3148\n",
    "Episode: 443 Total reward: 170.0 Training loss: 48.6270 Explore P: 0.3097\n",
    "Episode: 444 Total reward: 77.0 Training loss: 2.2555 Explore P: 0.3074\n",
    "Episode: 445 Total reward: 84.0 Training loss: 3.0428 Explore P: 0.3049\n",
    "Episode: 447 Total reward: 12.0 Training loss: 2.8022 Explore P: 0.2987\n",
    "Episode: 448 Total reward: 66.0 Training loss: 120.3442 Explore P: 0.2968\n",
    "Episode: 449 Total reward: 152.0 Training loss: 6.2880 Explore P: 0.2925\n",
    "Episode: 450 Total reward: 141.0 Training loss: 2.8015 Explore P: 0.2885\n",
    "Episode: 451 Total reward: 99.0 Training loss: 64.0921 Explore P: 0.2858\n",
    "Episode: 452 Total reward: 79.0 Training loss: 1.5581 Explore P: 0.2836\n",
    "Episode: 453 Total reward: 49.0 Training loss: 113.2557 Explore P: 0.2823\n",
    "Episode: 455 Total reward: 106.0 Training loss: 210.4934 Explore P: 0.2741\n",
    "Episode: 456 Total reward: 109.0 Training loss: 81.6662 Explore P: 0.2712\n",
    "Episode: 457 Total reward: 56.0 Training loss: 3.2287 Explore P: 0.2697\n",
    "Episode: 458 Total reward: 138.0 Training loss: 2.5795 Explore P: 0.2662\n",
    "Episode: 459 Total reward: 93.0 Training loss: 3.4260 Explore P: 0.2638\n",
    "Episode: 460 Total reward: 71.0 Training loss: 139.3341 Explore P: 0.2620\n",
    "Episode: 461 Total reward: 106.0 Training loss: 2.6074 Explore P: 0.2594\n",
    "Episode: 462 Total reward: 63.0 Training loss: 2.8252 Explore P: 0.2578\n",
    "Episode: 463 Total reward: 71.0 Training loss: 25.8917 Explore P: 0.2560\n",
    "Episode: 464 Total reward: 79.0 Training loss: 3.8067 Explore P: 0.2541\n",
    "Episode: 465 Total reward: 86.0 Training loss: 1.6050 Explore P: 0.2520\n",
    "Episode: 466 Total reward: 88.0 Training loss: 44.2827 Explore P: 0.2499\n",
    "Episode: 467 Total reward: 72.0 Training loss: 0.7160 Explore P: 0.2482\n",
    "Episode: 468 Total reward: 152.0 Training loss: 75.7239 Explore P: 0.2446\n",
    "Episode: 469 Total reward: 122.0 Training loss: 7.4345 Explore P: 0.2417\n",
    "Episode: 470 Total reward: 81.0 Training loss: 101.0922 Explore P: 0.2399\n",
    "Episode: 471 Total reward: 38.0 Training loss: 1.6301 Explore P: 0.2390\n",
    "Episode: 472 Total reward: 79.0 Training loss: 72.4920 Explore P: 0.2372\n",
    "Episode: 473 Total reward: 190.0 Training loss: 1.3869 Explore P: 0.2329\n",
    "Episode: 474 Total reward: 197.0 Training loss: 1.5386 Explore P: 0.2286\n",
    "Episode: 476 Total reward: 42.0 Training loss: 0.8364 Explore P: 0.2233\n",
    "Episode: 477 Total reward: 134.0 Training loss: 88.3979 Explore P: 0.2205\n",
    "Episode: 478 Total reward: 128.0 Training loss: 94.1007 Explore P: 0.2178\n",
    "Episode: 479 Total reward: 79.0 Training loss: 103.1366 Explore P: 0.2162\n",
    "Episode: 480 Total reward: 169.0 Training loss: 58.8788 Explore P: 0.2127\n",
    "Episode: 481 Total reward: 160.0 Training loss: 1.1934 Explore P: 0.2095\n",
    "Episode: 482 Total reward: 81.0 Training loss: 2.9244 Explore P: 0.2079\n",
    "Episode: 484 Total reward: 68.0 Training loss: 77.9688 Explore P: 0.2027\n",
    "Episode: 486 Total reward: 17.0 Training loss: 0.6864 Explore P: 0.1985\n",
    "Episode: 488 Total reward: 62.0 Training loss: 1.9978 Explore P: 0.1937\n",
    "Episode: 489 Total reward: 178.0 Training loss: 228.6335 Explore P: 0.1904\n",
    "Episode: 490 Total reward: 114.0 Training loss: 0.4453 Explore P: 0.1884\n",
    "Episode: 491 Total reward: 127.0 Training loss: 1.6523 Explore P: 0.1861\n",
    "Episode: 492 Total reward: 124.0 Training loss: 120.2207 Explore P: 0.1840\n",
    "Episode: 493 Total reward: 184.0 Training loss: 0.5913 Explore P: 0.1808\n",
    "Episode: 494 Total reward: 129.0 Training loss: 56.3829 Explore P: 0.1786\n",
    "Episode: 495 Total reward: 95.0 Training loss: 1.9883 Explore P: 0.1770\n",
    "Episode: 496 Total reward: 129.0 Training loss: 1.2513 Explore P: 0.1749\n",
    "Episode: 497 Total reward: 176.0 Training loss: 1.0322 Explore P: 0.1720\n",
    "Episode: 498 Total reward: 132.0 Training loss: 0.9320 Explore P: 0.1699\n",
    "Episode: 499 Total reward: 146.0 Training loss: 289.4379 Explore P: 0.1675\n",
    "Episode: 500 Total reward: 147.0 Training loss: 0.5124 Explore P: 0.1652\n",
    "Episode: 501 Total reward: 166.0 Training loss: 0.9444 Explore P: 0.1627\n",
    "Episode: 503 Total reward: 31.0 Training loss: 1.4756 Explore P: 0.1592\n",
    "Episode: 504 Total reward: 129.0 Training loss: 0.6077 Explore P: 0.1573\n",
    "Episode: 505 Total reward: 127.0 Training loss: 1.2508 Explore P: 0.1554\n",
    "Episode: 506 Total reward: 123.0 Training loss: 0.8265 Explore P: 0.1537\n",
    "Episode: 507 Total reward: 159.0 Training loss: 260.4604 Explore P: 0.1514\n",
    "Episode: 508 Total reward: 136.0 Training loss: 0.9311 Explore P: 0.1495\n",
    "Episode: 509 Total reward: 198.0 Training loss: 0.9262 Explore P: 0.1467\n",
    "Episode: 511 Total reward: 40.0 Training loss: 2.3126 Explore P: 0.1435\n",
    "Episode: 512 Total reward: 130.0 Training loss: 1.2985 Explore P: 0.1418\n",
    "Episode: 514 Total reward: 25.0 Training loss: 1.1655 Explore P: 0.1388\n",
    "Episode: 515 Total reward: 149.0 Training loss: 214.9246 Explore P: 0.1369\n",
    "Episode: 516 Total reward: 200.0 Training loss: 0.8085 Explore P: 0.1344\n",
    "Episode: 517 Total reward: 172.0 Training loss: 24.9451 Explore P: 0.1323\n",
    "Episode: 519 Total reward: 52.0 Training loss: 61.7503 Explore P: 0.1293\n",
    "Episode: 520 Total reward: 176.0 Training loss: 0.4361 Explore P: 0.1272\n",
    "Episode: 521 Total reward: 160.0 Training loss: 1.8377 Explore P: 0.1253\n",
    "Episode: 522 Total reward: 180.0 Training loss: 1.5684 Explore P: 0.1233\n",
    "Episode: 523 Total reward: 174.0 Training loss: 58.6258 Explore P: 0.1213\n",
    "Episode: 525 Total reward: 10.0 Training loss: 222.1836 Explore P: 0.1190\n",
    "Episode: 527 Total reward: 32.0 Training loss: 0.4007 Explore P: 0.1165\n",
    "Episode: 529 Total reward: 33.0 Training loss: 0.3054 Explore P: 0.1140\n",
    "Episode: 530 Total reward: 185.0 Training loss: 0.7425 Explore P: 0.1121\n",
    "Episode: 531 Total reward: 171.0 Training loss: 0.3441 Explore P: 0.1104\n",
    "Episode: 532 Total reward: 199.0 Training loss: 0.2333 Explore P: 0.1084\n",
    "Episode: 534 Total reward: 95.0 Training loss: 0.4929 Explore P: 0.1056\n",
    "Episode: 536 Total reward: 8.0 Training loss: 0.5416 Explore P: 0.1036\n",
    "Episode: 538 Total reward: 42.0 Training loss: 163.3946 Explore P: 0.1014\n",
    "Episode: 539 Total reward: 180.0 Training loss: 0.2803 Explore P: 0.0997\n",
    "Episode: 540 Total reward: 193.0 Training loss: 0.4929 Explore P: 0.0980\n",
    "Episode: 542 Total reward: 36.0 Training loss: 0.7983 Explore P: 0.0960\n",
    "Episode: 544 Total reward: 152.0 Training loss: 41.4165 Explore P: 0.0930\n",
    "Episode: 546 Total reward: 30.0 Training loss: 0.7570 Explore P: 0.0911\n",
    "Episode: 548 Total reward: 50.0 Training loss: 0.3215 Explore P: 0.0891\n",
    "Episode: 550 Total reward: 79.0 Training loss: 0.5349 Explore P: 0.0869\n",
    "Episode: 552 Total reward: 38.0 Training loss: 0.3635 Explore P: 0.0851\n",
    "Episode: 553 Total reward: 131.0 Training loss: 0.3965 Explore P: 0.0841\n",
    "Episode: 554 Total reward: 135.0 Training loss: 0.2453 Explore P: 0.0831\n",
    "Episode: 555 Total reward: 111.0 Training loss: 0.9434 Explore P: 0.0823\n",
    "Episode: 556 Total reward: 136.0 Training loss: 0.7058 Explore P: 0.0814\n",
    "Episode: 557 Total reward: 106.0 Training loss: 0.4755 Explore P: 0.0806\n",
    "Episode: 558 Total reward: 98.0 Training loss: 0.4107 Explore P: 0.0799\n",
    "Episode: 559 Total reward: 102.0 Training loss: 62.3874 Explore P: 0.0792\n",
    "Episode: 560 Total reward: 92.0 Training loss: 0.4026 Explore P: 0.0786\n",
    "Episode: 561 Total reward: 86.0 Training loss: 0.3649 Explore P: 0.0780\n",
    "Episode: 562 Total reward: 83.0 Training loss: 0.5843 Explore P: 0.0774\n",
    "Episode: 563 Total reward: 100.0 Training loss: 0.1493 Explore P: 0.0768\n",
    "Episode: 564 Total reward: 102.0 Training loss: 0.4021 Explore P: 0.0761\n",
    "Episode: 565 Total reward: 60.0 Training loss: 0.3445 Explore P: 0.0757\n",
    "Episode: 566 Total reward: 63.0 Training loss: 0.2461 Explore P: 0.0753\n",
    "Episode: 567 Total reward: 59.0 Training loss: 0.2115 Explore P: 0.0749\n",
    "Episode: 568 Total reward: 73.0 Training loss: 3.2738 Explore P: 0.0744\n",
    "Episode: 569 Total reward: 70.0 Training loss: 0.4267 Explore P: 0.0740\n",
    "Episode: 570 Total reward: 53.0 Training loss: 0.8779 Explore P: 0.0736\n",
    "Episode: 571 Total reward: 88.0 Training loss: 187.5536 Explore P: 0.0731\n",
    "Episode: 572 Total reward: 54.0 Training loss: 0.3208 Explore P: 0.0727\n",
    "Episode: 573 Total reward: 87.0 Training loss: 0.2894 Explore P: 0.0722\n",
    "Episode: 574 Total reward: 58.0 Training loss: 0.2578 Explore P: 0.0718\n",
    "Episode: 575 Total reward: 85.0 Training loss: 0.3401 Explore P: 0.0713\n",
    "Episode: 576 Total reward: 73.0 Training loss: 0.2245 Explore P: 0.0709\n",
    "Episode: 577 Total reward: 114.0 Training loss: 0.3640 Explore P: 0.0702\n",
    "Episode: 578 Total reward: 94.0 Training loss: 0.7954 Explore P: 0.0696\n",
    "Episode: 579 Total reward: 114.0 Training loss: 0.2615 Explore P: 0.0689\n",
    "Episode: 580 Total reward: 80.0 Training loss: 0.4812 Explore P: 0.0685\n",
    "Episode: 581 Total reward: 125.0 Training loss: 0.8818 Explore P: 0.0677\n",
    "Episode: 582 Total reward: 109.0 Training loss: 0.2953 Explore P: 0.0671\n",
    "Episode: 583 Total reward: 98.0 Training loss: 0.4371 Explore P: 0.0665\n",
    "Episode: 584 Total reward: 119.0 Training loss: 0.4685 Explore P: 0.0659\n",
    "Episode: 585 Total reward: 96.0 Training loss: 0.3440 Explore P: 0.0653\n",
    "Episode: 586 Total reward: 172.0 Training loss: 0.1414 Explore P: 0.0644\n",
    "Episode: 587 Total reward: 104.0 Training loss: 0.3309 Explore P: 0.0638\n",
    "Episode: 588 Total reward: 85.0 Training loss: 0.2262 Explore P: 0.0634\n",
    "Episode: 590 Total reward: 104.0 Training loss: 0.3231 Explore P: 0.0618\n",
    "Episode: 591 Total reward: 148.0 Training loss: 0.4431 Explore P: 0.0610\n",
    "Episode: 592 Total reward: 135.0 Training loss: 0.1894 Explore P: 0.0603\n",
    "Episode: 595 Total reward: 99.0 Training loss: 0.2376 Explore P: 0.0579\n",
    "Episode: 597 Total reward: 172.0 Training loss: 0.4083 Explore P: 0.0561\n",
    "Episode: 600 Total reward: 99.0 Training loss: 0.1152 Explore P: 0.0539\n",
    "Episode: 603 Total reward: 99.0 Training loss: 0.3594 Explore P: 0.0518\n",
    "Episode: 604 Total reward: 149.0 Training loss: 0.1398 Explore P: 0.0511\n",
    "Episode: 607 Total reward: 99.0 Training loss: 0.3337 Explore P: 0.0491\n",
    "Episode: 608 Total reward: 180.0 Training loss: 7.4786 Explore P: 0.0484\n",
    "Episode: 611 Total reward: 28.0 Training loss: 0.1953 Explore P: 0.0468\n",
    "Episode: 614 Total reward: 38.0 Training loss: 0.3152 Explore P: 0.0453\n",
    "Episode: 617 Total reward: 50.0 Training loss: 0.4420 Explore P: 0.0437\n",
    "Episode: 620 Total reward: 9.0 Training loss: 0.3347 Explore P: 0.0423\n",
    "Episode: 623 Total reward: 99.0 Training loss: 0.1405 Explore P: 0.0408\n",
    "Episode: 626 Total reward: 78.0 Training loss: 0.1488 Explore P: 0.0393\n",
    "Episode: 628 Total reward: 198.0 Training loss: 0.9185 Explore P: 0.0382\n",
    "Episode: 631 Total reward: 99.0 Training loss: 0.3505 Explore P: 0.0368\n",
    "Episode: 633 Total reward: 142.0 Training loss: 0.1654 Explore P: 0.0359\n",
    "Episode: 635 Total reward: 134.0 Training loss: 0.3178 Explore P: 0.0351\n",
    "Episode: 637 Total reward: 104.0 Training loss: 0.3331 Explore P: 0.0343\n",
    "Episode: 639 Total reward: 73.0 Training loss: 66.8497 Explore P: 0.0337\n",
    "Episode: 641 Total reward: 35.0 Training loss: 0.1411 Explore P: 0.0331\n",
    "Episode: 643 Total reward: 83.0 Training loss: 0.2136 Explore P: 0.0325\n",
    "Episode: 645 Total reward: 62.0 Training loss: 0.2303 Explore P: 0.0319\n",
    "Episode: 647 Total reward: 28.0 Training loss: 0.2552 Explore P: 0.0314\n",
    "Episode: 649 Total reward: 4.0 Training loss: 0.1967 Explore P: 0.0310\n",
    "Episode: 650 Total reward: 194.0 Training loss: 0.1424 Explore P: 0.0306\n",
    "Episode: 651 Total reward: 170.0 Training loss: 0.1509 Explore P: 0.0302\n",
    "Episode: 652 Total reward: 150.0 Training loss: 0.2699 Explore P: 0.0299\n",
    "Episode: 653 Total reward: 161.0 Training loss: 0.2821 Explore P: 0.0296\n",
    "Episode: 654 Total reward: 148.0 Training loss: 0.4859 Explore P: 0.0293\n",
    "Episode: 655 Total reward: 142.0 Training loss: 0.1541 Explore P: 0.0290\n",
    "Episode: 656 Total reward: 140.0 Training loss: 0.0963 Explore P: 0.0288\n",
    "Episode: 657 Total reward: 144.0 Training loss: 0.3165 Explore P: 0.0285\n",
    "Episode: 658 Total reward: 160.0 Training loss: 0.2059 Explore P: 0.0282\n",
    "Episode: 659 Total reward: 127.0 Training loss: 0.0918 Explore P: 0.0280\n",
    "Episode: 660 Total reward: 124.0 Training loss: 431.4700 Explore P: 0.0278\n",
    "Episode: 661 Total reward: 127.0 Training loss: 0.2660 Explore P: 0.0275\n",
    "Episode: 662 Total reward: 133.0 Training loss: 0.4122 Explore P: 0.0273\n",
    "Episode: 663 Total reward: 119.0 Training loss: 0.2070 Explore P: 0.0271\n",
    "Episode: 664 Total reward: 114.0 Training loss: 0.3453 Explore P: 0.0269\n",
    "Episode: 665 Total reward: 130.0 Training loss: 0.3865 Explore P: 0.0267\n",
    "Episode: 666 Total reward: 125.0 Training loss: 0.2518 Explore P: 0.0265\n",
    "Episode: 667 Total reward: 138.0 Training loss: 0.1668 Explore P: 0.0263\n",
    "Episode: 669 Total reward: 42.0 Training loss: 0.3241 Explore P: 0.0259\n",
    "Episode: 671 Total reward: 105.0 Training loss: 0.1787 Explore P: 0.0254\n",
    "Episode: 674 Total reward: 99.0 Training loss: 0.2393 Explore P: 0.0246\n",
    "Episode: 677 Total reward: 99.0 Training loss: 0.2190 Explore P: 0.0239\n",
    "Episode: 680 Total reward: 99.0 Training loss: 2.5996 Explore P: 0.0232\n",
    "Episode: 683 Total reward: 99.0 Training loss: 0.3376 Explore P: 0.0226\n",
    "Episode: 686 Total reward: 99.0 Training loss: 0.5884 Explore P: 0.0220\n",
    "Episode: 689 Total reward: 99.0 Training loss: 0.1356 Explore P: 0.0214\n",
    "Episode: 692 Total reward: 99.0 Training loss: 0.0920 Explore P: 0.0208\n",
    "Episode: 695 Total reward: 99.0 Training loss: 0.1880 Explore P: 0.0203\n",
    "Episode: 698 Total reward: 99.0 Training loss: 0.0951 Explore P: 0.0198\n",
    "Episode: 701 Total reward: 99.0 Training loss: 0.1050 Explore P: 0.0193\n",
    "Episode: 704 Total reward: 99.0 Training loss: 0.1234 Explore P: 0.0189\n",
    "Episode: 707 Total reward: 99.0 Training loss: 0.1407 Explore P: 0.0185\n",
    "Episode: 709 Total reward: 160.0 Training loss: 0.0913 Explore P: 0.0182\n",
    "Episode: 712 Total reward: 99.0 Training loss: 0.1815 Explore P: 0.0178\n",
    "Episode: 715 Total reward: 99.0 Training loss: 0.1191 Explore P: 0.0174\n",
    "Episode: 718 Total reward: 99.0 Training loss: 0.1073 Explore P: 0.0170\n",
    "Episode: 721 Total reward: 99.0 Training loss: 0.1133 Explore P: 0.0167\n",
    "Episode: 724 Total reward: 99.0 Training loss: 0.0898 Explore P: 0.0164\n",
    "Episode: 727 Total reward: 99.0 Training loss: 0.1217 Explore P: 0.0160\n",
    "Episode: 730 Total reward: 99.0 Training loss: 0.2150 Explore P: 0.0158\n",
    "Episode: 733 Total reward: 99.0 Training loss: 0.0678 Explore P: 0.0155\n",
    "Episode: 736 Total reward: 99.0 Training loss: 0.0872 Explore P: 0.0152\n",
    "Episode: 739 Total reward: 99.0 Training loss: 0.1330 Explore P: 0.0150\n",
    "Episode: 742 Total reward: 99.0 Training loss: 0.1116 Explore P: 0.0147\n",
    "Episode: 745 Total reward: 99.0 Training loss: 0.1611 Explore P: 0.0145\n",
    "Episode: 748 Total reward: 99.0 Training loss: 0.1307 Explore P: 0.0143\n",
    "Episode: 751 Total reward: 99.0 Training loss: 0.0875 Explore P: 0.0141\n",
    "Episode: 754 Total reward: 99.0 Training loss: 0.1344 Explore P: 0.0139\n",
    "Episode: 757 Total reward: 99.0 Training loss: 0.0911 Explore P: 0.0137\n",
    "Episode: 760 Total reward: 99.0 Training loss: 0.1224 Explore P: 0.0135\n",
    "Episode: 763 Total reward: 99.0 Training loss: 0.0572 Explore P: 0.0133\n",
    "Episode: 766 Total reward: 99.0 Training loss: 0.0757 Explore P: 0.0132\n",
    "Episode: 769 Total reward: 99.0 Training loss: 0.0381 Explore P: 0.0130\n",
    "Episode: 772 Total reward: 99.0 Training loss: 0.1698 Explore P: 0.0129\n",
    "Episode: 775 Total reward: 99.0 Training loss: 0.0365 Explore P: 0.0127\n",
    "Episode: 778 Total reward: 99.0 Training loss: 0.1805 Explore P: 0.0126\n",
    "Episode: 781 Total reward: 99.0 Training loss: 0.1017 Explore P: 0.0125\n",
    "Episode: 784 Total reward: 99.0 Training loss: 0.1112 Explore P: 0.0123\n",
    "Episode: 787 Total reward: 99.0 Training loss: 0.0930 Explore P: 0.0122\n",
    "Episode: 790 Total reward: 99.0 Training loss: 0.0693 Explore P: 0.0121\n",
    "Episode: 793 Total reward: 99.0 Training loss: 0.0431 Explore P: 0.0120\n",
    "Episode: 796 Total reward: 99.0 Training loss: 0.1168 Explore P: 0.0119\n",
    "Episode: 799 Total reward: 99.0 Training loss: 0.1071 Explore P: 0.0118\n",
    "Episode: 802 Total reward: 99.0 Training loss: 0.1360 Explore P: 0.0117\n",
    "Episode: 805 Total reward: 99.0 Training loss: 0.0872 Explore P: 0.0117\n",
    "Episode: 808 Total reward: 99.0 Training loss: 0.1197 Explore P: 0.0116\n",
    "Episode: 811 Total reward: 99.0 Training loss: 0.0848 Explore P: 0.0115\n",
    "Episode: 814 Total reward: 99.0 Training loss: 0.0515 Explore P: 0.0114\n",
    "Episode: 817 Total reward: 99.0 Training loss: 0.1590 Explore P: 0.0114\n",
    "Episode: 820 Total reward: 99.0 Training loss: 0.2080 Explore P: 0.0113\n",
    "Episode: 823 Total reward: 99.0 Training loss: 0.1532 Explore P: 0.0112\n",
    "Episode: 826 Total reward: 99.0 Training loss: 0.0622 Explore P: 0.0112\n",
    "Episode: 829 Total reward: 99.0 Training loss: 0.0553 Explore P: 0.0111\n",
    "Episode: 832 Total reward: 99.0 Training loss: 0.0746 Explore P: 0.0111\n",
    "Episode: 835 Total reward: 99.0 Training loss: 0.1045 Explore P: 0.0110\n",
    "Episode: 838 Total reward: 99.0 Training loss: 0.0929 Explore P: 0.0110\n",
    "Episode: 841 Total reward: 99.0 Training loss: 0.1053 Explore P: 0.0109\n",
    "Episode: 844 Total reward: 99.0 Training loss: 0.0877 Explore P: 0.0109\n",
    "Episode: 847 Total reward: 99.0 Training loss: 0.0783 Explore P: 0.0108\n",
    "Episode: 850 Total reward: 99.0 Training loss: 0.0724 Explore P: 0.0108\n",
    "Episode: 853 Total reward: 99.0 Training loss: 0.1745 Explore P: 0.0107\n",
    "Episode: 856 Total reward: 99.0 Training loss: 0.0334 Explore P: 0.0107\n",
    "Episode: 859 Total reward: 99.0 Training loss: 0.0205 Explore P: 0.0107\n",
    "Episode: 862 Total reward: 99.0 Training loss: 0.0674 Explore P: 0.0106\n",
    "Episode: 865 Total reward: 99.0 Training loss: 0.1149 Explore P: 0.0106\n",
    "Episode: 868 Total reward: 99.0 Training loss: 191.0773 Explore P: 0.0106\n",
    "Episode: 871 Total reward: 99.0 Training loss: 0.1013 Explore P: 0.0106\n",
    "Episode: 873 Total reward: 156.0 Training loss: 0.2140 Explore P: 0.0105\n",
    "Episode: 874 Total reward: 185.0 Training loss: 0.1837 Explore P: 0.0105\n",
    "Episode: 876 Total reward: 45.0 Training loss: 0.1855 Explore P: 0.0105\n",
    "Episode: 877 Total reward: 55.0 Training loss: 0.0789 Explore P: 0.0105\n",
    "Episode: 880 Total reward: 99.0 Training loss: 0.0890 Explore P: 0.0105\n",
    "Episode: 882 Total reward: 185.0 Training loss: 0.0788 Explore P: 0.0105\n",
    "Episode: 885 Total reward: 99.0 Training loss: 0.1397 Explore P: 0.0104\n",
    "Episode: 888 Total reward: 99.0 Training loss: 0.0400 Explore P: 0.0104\n",
    "Episode: 891 Total reward: 99.0 Training loss: 0.0962 Explore P: 0.0104\n",
    "Episode: 894 Total reward: 99.0 Training loss: 0.1356 Explore P: 0.0104\n",
    "Episode: 897 Total reward: 99.0 Training loss: 0.2037 Explore P: 0.0104\n",
    "Episode: 900 Total reward: 99.0 Training loss: 0.0486 Explore P: 0.0103\n",
    "Episode: 903 Total reward: 99.0 Training loss: 0.2492 Explore P: 0.0103\n",
    "Episode: 906 Total reward: 99.0 Training loss: 0.1467 Explore P: 0.0103\n",
    "Episode: 909 Total reward: 99.0 Training loss: 0.2217 Explore P: 0.0103\n",
    "Episode: 912 Total reward: 99.0 Training loss: 0.1772 Explore P: 0.0103\n",
    "Episode: 915 Total reward: 99.0 Training loss: 0.0898 Explore P: 0.0103\n",
    "Episode: 918 Total reward: 99.0 Training loss: 0.0552 Explore P: 0.0103\n",
    "Episode: 921 Total reward: 99.0 Training loss: 0.1267 Explore P: 0.0102\n",
    "Episode: 924 Total reward: 99.0 Training loss: 0.3037 Explore P: 0.0102\n",
    "Episode: 927 Total reward: 99.0 Training loss: 0.1654 Explore P: 0.0102\n",
    "Episode: 930 Total reward: 99.0 Training loss: 0.1975 Explore P: 0.0102\n",
    "Episode: 933 Total reward: 99.0 Training loss: 0.2122 Explore P: 0.0102\n",
    "Episode: 936 Total reward: 99.0 Training loss: 0.0754 Explore P: 0.0102\n",
    "Episode: 939 Total reward: 99.0 Training loss: 0.1481 Explore P: 0.0102\n",
    "Episode: 942 Total reward: 99.0 Training loss: 0.0895 Explore P: 0.0102\n",
    "Episode: 945 Total reward: 99.0 Training loss: 0.0690 Explore P: 0.0102\n",
    "Episode: 948 Total reward: 99.0 Training loss: 0.0942 Explore P: 0.0102\n",
    "Episode: 951 Total reward: 99.0 Training loss: 0.0567 Explore P: 0.0101\n",
    "Episode: 954 Total reward: 99.0 Training loss: 0.0665 Explore P: 0.0101\n",
    "Episode: 957 Total reward: 99.0 Training loss: 0.0645 Explore P: 0.0101\n",
    "Episode: 960 Total reward: 99.0 Training loss: 224.4461 Explore P: 0.0101\n",
    "Episode: 963 Total reward: 99.0 Training loss: 0.0508 Explore P: 0.0101\n",
    "Episode: 966 Total reward: 99.0 Training loss: 0.0792 Explore P: 0.0101\n",
    "Episode: 969 Total reward: 99.0 Training loss: 0.0754 Explore P: 0.0101\n",
    "Episode: 972 Total reward: 99.0 Training loss: 0.0655 Explore P: 0.0101\n",
    "Episode: 975 Total reward: 99.0 Training loss: 0.0686 Explore P: 0.0101\n",
    "Episode: 978 Total reward: 99.0 Training loss: 0.0361 Explore P: 0.0101\n",
    "Episode: 981 Total reward: 99.0 Training loss: 0.1777 Explore P: 0.0101\n",
    "Episode: 984 Total reward: 99.0 Training loss: 0.0633 Explore P: 0.0101\n",
    "Episode: 987 Total reward: 99.0 Training loss: 0.0559 Explore P: 0.0101\n",
    "Episode: 990 Total reward: 99.0 Training loss: 0.0543 Explore P: 0.0101\n",
    "Episode: 993 Total reward: 99.0 Training loss: 0.0833 Explore P: 0.0101\n",
    "Episode: 996 Total reward: 99.0 Training loss: 0.1037 Explore P: 0.0101\n",
    "Episode: 997 Total reward: 45.0 Training loss: 0.0619 Explore P: 0.0101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化训练结果\n",
    "\n",
    "我们在下面绘制了每个阶段的总奖励。滚动平均值用蓝色表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text(0,0.5,'Total Reward')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_21_1.png)\n",
    "\n",
    "\n",
    "## 玩街机游戏\n",
    "\n",
    "Cart-Pole 是一个非常简单的游戏。但是，可以使用同一模型训练智能体玩非常复杂的游戏，例如 Pong 或 Space Invaders。你需要使用卷积层从屏幕图片上获取状态，而不是采取在此游戏中使用的状态。\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "作为一项挑战，我将请你来使用深度 Q 学习训练智能体玩街机游戏。为了获得指导，请参阅以下原始论文：https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/nature14236.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
